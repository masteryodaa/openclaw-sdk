# v0.2.0 Implementation Plan — "LangChain for OpenClaw"

> **For Claude:** REQUIRED SUB-SKILL: Use superpowers:executing-plans to implement this plan task-by-task.

**Goal:** Make openclaw-sdk the definitive Python SDK for OpenClaw — what LangChain is for LLMs, this SDK is for OpenClaw agents.

**Architecture:** Extend v0.1's thin gateway wrapper with Python-native power features: response caching, hierarchical tracing, batch execution, prompt templates, evaluation framework, and fix protocol gaps discovered via live gateway testing.

**Tech Stack:** Python 3.11+, Pydantic v2, asyncio, structlog, pytest, optional redis for caching, optional opentelemetry-api for tracing export.

---

## Research Findings (verified 2026-02-21)

### New Gateway Methods (verified against live OpenClaw 2026.2.3-1)

| Method | Params | Status | Action |
|--------|--------|--------|--------|
| `agent.wait` | `{runId}` | **EXISTS** | Add facade + Agent.wait_for_run() |
| `exec.approval.resolve` | `{id, decision}` | **EXISTS** | Fix ApprovalManager (currently raises NotImplementedError!) |
| `device.token.rotate` | `{deviceId, role}` | **EXISTS** | Add DeviceManager |
| `device.token.revoke` | `{deviceId, role}` | **EXISTS** | Add DeviceManager |
| `skills.bins` | (elevated role) | **EXISTS** | Add to SkillManager |

### Medium Priority — Confirmed OpenClaw-Native (SKIP)

| Feature | OpenClaw Native? | SDK Action |
|---------|-----------------|------------|
| RAG / Document Processing | YES — built-in file reading, web search, memory | Skip — OpenClaw handles it |
| Retry / Fallback | YES — full failover chain (model.fallbacks) | Skip — OpenClaw handles it |
| Docker Management | YES — SandboxContext, auto-lifecycle | Skip — OpenClaw handles it |
| Streaming | YES — WebSocket events | Already wrapped |
| Conditional Workflow | PARTIAL — subagents + routing, no DAG | Keep Pipeline, enhance later |

---

## Task Breakdown

### Task 1: Fix ApprovalManager — `exec.approval.resolve` EXISTS

**Priority:** CRITICAL — current code is wrong, tells users the method doesn't exist when it does.

**Files:**
- Modify: `src/openclaw_sdk/gateway/base.py:228-253` (approval facade methods)
- Modify: `src/openclaw_sdk/approvals/manager.py` (full rewrite)
- Test: `tests/unit/test_approval_manager.py` (rewrite)
- Modify: `docs/protocol.md` (update approvals section)

**Step 1: Write the failing test**

```python
# tests/unit/test_approval_resolve.py
import pytest
from openclaw_sdk.gateway.mock import MockGateway
from openclaw_sdk.approvals.manager import ApprovalManager


async def test_resolve_approval_calls_gateway():
    gw = MockGateway()
    await gw.connect()
    gw.register("exec.approval.resolve", {"ok": True})
    mgr = ApprovalManager(gw)
    result = await mgr.resolve("req_123", "approve")
    assert result == {"ok": True}


async def test_resolve_approval_with_deny():
    gw = MockGateway()
    await gw.connect()
    gw.register("exec.approval.resolve", {"ok": True})
    mgr = ApprovalManager(gw)
    result = await mgr.resolve("req_456", "deny")
    assert result == {"ok": True}
```

**Step 2: Run test to verify it fails**

Run: `python -m pytest tests/unit/test_approval_resolve.py -v`
Expected: FAIL — current code raises NotImplementedError

**Step 3: Rewrite ApprovalManager**

```python
# src/openclaw_sdk/approvals/manager.py
"""ApprovalManager — wrapper for execution-approval workflows.

Approvals in OpenClaw are delivered as push events (``approval.requested``)
via ``Gateway.subscribe()``. Once received, resolve them via
``exec.approval.resolve``.
"""
from __future__ import annotations

from typing import Any, Literal

from openclaw_sdk.gateway.base import GatewayProtocol


class ApprovalManager:
    """Manage pending execution approvals.

    When an agent operates in ``"confirm"`` permission mode, it pauses before
    executing dangerous tools and emits an ``approval.requested`` push event.

    **Workflow:**
    1. Subscribe to gateway events and listen for ``approval.requested``
    2. Call :meth:`resolve` with the request ``id`` and a ``decision``

    Example::

        async for event in await client.gateway.subscribe():
            if event.event_type == "approval.requested":
                req_id = event.data["payload"]["id"]
                await client.approvals.resolve(req_id, "approve")
    """

    def __init__(self, gateway: GatewayProtocol) -> None:
        self._gateway = gateway

    async def resolve(
        self,
        request_id: str,
        decision: Literal["approve", "deny"],
    ) -> dict[str, Any]:
        """Approve or deny a pending execution request.

        Gateway method: ``exec.approval.resolve``
        Verified params: ``{id, decision}``

        Args:
            request_id: The approval request ID from the push event.
            decision: ``"approve"`` or ``"deny"``.

        Returns:
            Gateway response dict.
        """
        return await self._gateway.call(
            "exec.approval.resolve",
            {"id": request_id, "decision": decision},
        )
```

**Step 4: Update gateway facade**

Replace the two `NotImplementedError` methods in `gateway/base.py` with:

```python
async def resolve_approval(
    self,
    request_id: str,
    decision: str,
) -> dict[str, Any]:
    """Approve or deny a pending execution request.

    Gateway method: ``exec.approval.resolve``
    Verified params: ``{id, decision}``
    """
    return await self.call(
        "exec.approval.resolve",
        {"id": request_id, "decision": decision},
    )
```

Remove `list_approval_requests()` entirely (no such RPC exists).

**Step 5: Run tests, verify pass**

Run: `python -m pytest tests/unit/test_approval_resolve.py tests/unit/test_approval_manager.py -v`

**Step 6: Commit**

```bash
git add src/openclaw_sdk/approvals/ src/openclaw_sdk/gateway/base.py tests/unit/test_approval_resolve.py tests/unit/test_approval_manager.py
git commit -m "fix(approvals): exec.approval.resolve exists — rewrite ApprovalManager"
```

---

### Task 2: Add `agent.wait` facade + `Agent.wait_for_run()`

**Priority:** HIGH — enables non-blocking execution pattern (send + poll).

**Files:**
- Modify: `src/openclaw_sdk/gateway/base.py` (add `agent_wait` facade)
- Modify: `src/openclaw_sdk/core/agent.py` (add `wait_for_run` method)
- Test: `tests/unit/test_agent_wait.py`

**Step 1: Write the failing test**

```python
# tests/unit/test_agent_wait.py
import pytest
from openclaw_sdk import OpenClawClient, ClientConfig
from openclaw_sdk.gateway.mock import MockGateway


async def test_agent_wait_for_run():
    mock = MockGateway()
    await mock.connect()
    mock.register("agent.wait", {"status": "completed", "content": "Result here"})
    client = OpenClawClient(config=ClientConfig(), gateway=mock)
    agent = client.get_agent("bot")
    result = await agent.wait_for_run("run_abc123")
    assert result["status"] == "completed"
    assert result["content"] == "Result here"


async def test_gateway_agent_wait_facade():
    mock = MockGateway()
    await mock.connect()
    mock.register("agent.wait", {"status": "completed"})
    result = await mock.agent_wait("run_abc123")
    assert result["status"] == "completed"
```

**Step 2: Run test to verify it fails**

Run: `python -m pytest tests/unit/test_agent_wait.py -v`
Expected: FAIL — `agent_wait` and `wait_for_run` don't exist

**Step 3: Implement gateway facade**

Add to `gateway/base.py`:

```python
async def agent_wait(self, run_id: str) -> dict[str, Any]:
    """Wait for an agent run to complete.

    Gateway method: ``agent.wait``
    Verified param: ``{runId}``

    Args:
        run_id: The run ID returned by ``chat.send``.

    Returns:
        Gateway response with run result.
    """
    return await self.call("agent.wait", {"runId": run_id})
```

**Step 4: Implement Agent.wait_for_run()**

Add to `core/agent.py`:

```python
async def wait_for_run(self, run_id: str) -> dict[str, Any]:
    """Wait for a specific run to complete.

    Gateway method: ``agent.wait``

    This pairs with the ``runId`` returned by ``chat.send`` and enables
    a non-blocking execution pattern::

        send_result = await client.gateway.call("chat.send", {...})
        run_id = send_result["runId"]
        # ... do other work ...
        result = await agent.wait_for_run(run_id)

    Args:
        run_id: The run ID from a ``chat.send`` response.

    Returns:
        Gateway response dict with run result.
    """
    return await self._client.gateway.call("agent.wait", {"runId": run_id})
```

**Step 5: Run tests, verify pass**

Run: `python -m pytest tests/unit/test_agent_wait.py -v`

**Step 6: Commit**

```bash
git add src/openclaw_sdk/gateway/base.py src/openclaw_sdk/core/agent.py tests/unit/test_agent_wait.py
git commit -m "feat: add agent.wait facade + Agent.wait_for_run()"
```

---

### Task 3: Add DeviceManager (`device.token.rotate`, `device.token.revoke`)

**Priority:** HIGH — security feature for multi-device setups.

**Files:**
- Create: `src/openclaw_sdk/devices/` (new module)
- Create: `src/openclaw_sdk/devices/__init__.py`
- Create: `src/openclaw_sdk/devices/manager.py`
- Modify: `src/openclaw_sdk/core/client.py` (add `client.devices` property)
- Modify: `src/openclaw_sdk/gateway/base.py` (add device facade methods)
- Test: `tests/unit/test_device_manager.py`

**Step 1: Write the failing test**

```python
# tests/unit/test_device_manager.py
import pytest
from openclaw_sdk.gateway.mock import MockGateway
from openclaw_sdk.devices.manager import DeviceManager


async def test_rotate_token():
    gw = MockGateway()
    await gw.connect()
    gw.register("device.token.rotate", {"token": "new_tok_xyz", "expiresAt": "2026-03-01"})
    mgr = DeviceManager(gw)
    result = await mgr.rotate_token("device_abc", "operator")
    assert result["token"] == "new_tok_xyz"


async def test_revoke_token():
    gw = MockGateway()
    await gw.connect()
    gw.register("device.token.revoke", {"ok": True})
    mgr = DeviceManager(gw)
    result = await mgr.revoke_token("device_abc", "operator")
    assert result["ok"] is True
```

**Step 2: Run test to verify it fails**

Run: `python -m pytest tests/unit/test_device_manager.py -v`

**Step 3: Implement DeviceManager**

```python
# src/openclaw_sdk/devices/manager.py
"""DeviceManager — manage device tokens and pairing."""
from __future__ import annotations

from typing import Any

from openclaw_sdk.gateway.base import GatewayProtocol


class DeviceManager:
    """Manage device tokens for multi-device access.

    OpenClaw supports multiple connected devices (CLI, web UI, mobile nodes).
    Each device has an auth token that can be rotated or revoked.

    Example::

        await client.devices.rotate_token("my-phone", "node")
        await client.devices.revoke_token("old-laptop", "operator")
    """

    def __init__(self, gateway: GatewayProtocol) -> None:
        self._gateway = gateway

    async def rotate_token(
        self, device_id: str, role: str
    ) -> dict[str, Any]:
        """Rotate the auth token for a device.

        Gateway method: ``device.token.rotate``
        Verified params: ``{deviceId, role}``

        Args:
            device_id: The device identifier.
            role: The device role (``"operator"`` or ``"node"``).

        Returns:
            Gateway response with new token details.
        """
        return await self._gateway.call(
            "device.token.rotate",
            {"deviceId": device_id, "role": role},
        )

    async def revoke_token(
        self, device_id: str, role: str
    ) -> dict[str, Any]:
        """Revoke the auth token for a device.

        Gateway method: ``device.token.revoke``
        Verified params: ``{deviceId, role}``

        Args:
            device_id: The device identifier.
            role: The device role (``"operator"`` or ``"node"``).

        Returns:
            Gateway response confirming revocation.
        """
        return await self._gateway.call(
            "device.token.revoke",
            {"deviceId": device_id, "role": role},
        )
```

**Step 4: Wire into client**

Add to `core/client.py`:

```python
@property
def devices(self) -> DeviceManager:
    if self._devices is None:
        from openclaw_sdk.devices.manager import DeviceManager
        self._devices = DeviceManager(self._gateway)
    return self._devices
```

**Step 5: Run tests, verify pass**

Run: `python -m pytest tests/unit/test_device_manager.py -v`

**Step 6: Commit**

```bash
git add src/openclaw_sdk/devices/ src/openclaw_sdk/core/client.py src/openclaw_sdk/gateway/base.py tests/unit/test_device_manager.py
git commit -m "feat: add DeviceManager (device.token.rotate, device.token.revoke)"
```

---

### Task 4: Response Cache

**Priority:** HIGH — reduces gateway round-trips for repeated queries.

**Files:**
- Create: `src/openclaw_sdk/cache/` (new module)
- Create: `src/openclaw_sdk/cache/__init__.py`
- Create: `src/openclaw_sdk/cache/base.py` (ABC + InMemoryCache)
- Modify: `src/openclaw_sdk/core/agent.py` (cache integration in execute())
- Modify: `src/openclaw_sdk/core/client.py` (accept cache param)
- Test: `tests/unit/test_cache.py`

**Step 1: Write the failing test**

```python
# tests/unit/test_cache.py
import pytest
from openclaw_sdk.cache.base import InMemoryCache
from openclaw_sdk.core.types import ExecutionResult


async def test_cache_miss_returns_none():
    cache = InMemoryCache()
    result = await cache.get("bot", "hello")
    assert result is None


async def test_cache_hit_returns_result():
    cache = InMemoryCache()
    er = ExecutionResult(success=True, content="cached response")
    await cache.set("bot", "hello", er)
    result = await cache.get("bot", "hello")
    assert result is not None
    assert result.content == "cached response"


async def test_cache_ttl_expiry():
    cache = InMemoryCache(ttl_seconds=0)  # Immediate expiry
    er = ExecutionResult(success=True, content="cached response")
    await cache.set("bot", "hello", er)
    import asyncio
    await asyncio.sleep(0.01)
    result = await cache.get("bot", "hello")
    assert result is None


async def test_cache_clear():
    cache = InMemoryCache()
    er = ExecutionResult(success=True, content="cached response")
    await cache.set("bot", "hello", er)
    await cache.clear()
    result = await cache.get("bot", "hello")
    assert result is None


async def test_cache_max_size_eviction():
    cache = InMemoryCache(max_size=2)
    for i in range(3):
        er = ExecutionResult(success=True, content=f"response {i}")
        await cache.set("bot", f"query {i}", er)
    # First entry should be evicted
    assert await cache.get("bot", "query 0") is None
    assert await cache.get("bot", "query 2") is not None
```

**Step 2: Run test to verify it fails**

**Step 3: Implement cache**

```python
# src/openclaw_sdk/cache/base.py
"""Response cache for reducing gateway round-trips."""
from __future__ import annotations

import hashlib
import time
from abc import ABC, abstractmethod
from collections import OrderedDict
from typing import Any

from openclaw_sdk.core.types import ExecutionResult


class ResponseCache(ABC):
    """Abstract base for response caching."""

    @abstractmethod
    async def get(self, agent_id: str, query: str) -> ExecutionResult | None: ...

    @abstractmethod
    async def set(self, agent_id: str, query: str, result: ExecutionResult) -> None: ...

    @abstractmethod
    async def clear(self) -> None: ...

    @staticmethod
    def _cache_key(agent_id: str, query: str) -> str:
        raw = f"{agent_id}:{query}"
        return hashlib.sha256(raw.encode()).hexdigest()


class InMemoryCache(ResponseCache):
    """In-memory LRU cache with TTL.

    Args:
        ttl_seconds: Time-to-live for cache entries (default 300 = 5 minutes).
            Set to 0 to disable caching (entries expire immediately).
        max_size: Maximum number of cached entries (default 1000).
            Oldest entries evicted when full (LRU).
    """

    def __init__(self, ttl_seconds: int = 300, max_size: int = 1000) -> None:
        self._ttl = ttl_seconds
        self._max_size = max_size
        self._store: OrderedDict[str, tuple[float, ExecutionResult]] = OrderedDict()

    async def get(self, agent_id: str, query: str) -> ExecutionResult | None:
        key = self._cache_key(agent_id, query)
        entry = self._store.get(key)
        if entry is None:
            return None
        ts, result = entry
        if self._ttl > 0 and (time.monotonic() - ts) > self._ttl:
            del self._store[key]
            return None
        self._store.move_to_end(key)
        return result

    async def set(self, agent_id: str, query: str, result: ExecutionResult) -> None:
        key = self._cache_key(agent_id, query)
        self._store[key] = (time.monotonic(), result)
        self._store.move_to_end(key)
        while len(self._store) > self._max_size:
            self._store.popitem(last=False)

    async def clear(self) -> None:
        self._store.clear()
```

**Step 4: Wire into Agent.execute()**

Add optional `cache` parameter to `OpenClawClient.__init__()` and check cache in `Agent.execute()`:

```python
# In Agent.execute(), before gateway call:
if self._client._cache is not None:
    cached = await self._client._cache.get(self.agent_id, query)
    if cached is not None:
        await resolved_cbs.on_execution_end(self.agent_id, cached)
        return cached

# After successful execution, before returning:
if self._client._cache is not None and result.success:
    await self._client._cache.set(self.agent_id, query, result)
```

**Step 5: Run full test suite**

Run: `python -m pytest tests/ -q`

**Step 6: Commit**

```bash
git add src/openclaw_sdk/cache/ src/openclaw_sdk/core/agent.py src/openclaw_sdk/core/client.py tests/unit/test_cache.py
git commit -m "feat: add response cache (InMemoryCache with TTL + LRU eviction)"
```

---

### Task 5: Tracing / Observability

**Priority:** HIGH — the #1 feature teams need for production (per LangSmith survey: 69% of teams).

**Files:**
- Create: `src/openclaw_sdk/tracing/` (new module)
- Create: `src/openclaw_sdk/tracing/__init__.py`
- Create: `src/openclaw_sdk/tracing/tracer.py`
- Create: `src/openclaw_sdk/tracing/span.py`
- Modify: `src/openclaw_sdk/callbacks/handler.py` (add TracingCallbackHandler)
- Test: `tests/unit/test_tracing.py`

**Step 1: Write the failing test**

```python
# tests/unit/test_tracing.py
import pytest
from openclaw_sdk.tracing.tracer import Tracer
from openclaw_sdk.tracing.span import Span


async def test_tracer_creates_root_span():
    tracer = Tracer()
    span = tracer.start_span("agent.execute", agent_id="bot", query="hello")
    assert span.name == "agent.execute"
    assert span.parent_id is None
    assert span.agent_id == "bot"
    span.end()


async def test_tracer_creates_child_span():
    tracer = Tracer()
    root = tracer.start_span("agent.execute", agent_id="bot")
    child = tracer.start_span("chat.send", parent=root, agent_id="bot")
    assert child.parent_id == root.span_id
    child.end()
    root.end()


async def test_tracer_collects_completed_traces():
    tracer = Tracer()
    span = tracer.start_span("agent.execute", agent_id="bot")
    span.set_attribute("tokens.input", 100)
    span.set_attribute("tokens.output", 50)
    span.end()
    traces = tracer.get_traces()
    assert len(traces) == 1
    assert traces[0].attributes["tokens.input"] == 100


async def test_tracer_export_json():
    tracer = Tracer()
    span = tracer.start_span("agent.execute", agent_id="bot")
    span.end()
    data = tracer.export_json()
    assert isinstance(data, list)
    assert len(data) == 1
    assert data[0]["name"] == "agent.execute"


async def test_tracing_callback_handler():
    from openclaw_sdk.tracing.tracer import TracingCallbackHandler
    tracer = Tracer()
    handler = TracingCallbackHandler(tracer)
    await handler.on_execution_start("bot", "hello")
    from openclaw_sdk.core.types import ExecutionResult
    result = ExecutionResult(success=True, content="world", latency_ms=100)
    await handler.on_execution_end("bot", result)
    traces = tracer.get_traces()
    assert len(traces) == 1
    assert traces[0].attributes["latency_ms"] == 100
```

**Step 2: Run test to verify it fails**

**Step 3: Implement Span**

```python
# src/openclaw_sdk/tracing/span.py
"""Trace span — a single unit of work in an execution trace."""
from __future__ import annotations

import time
import uuid
from typing import Any


class Span:
    """Represents a single span in an execution trace.

    Spans form a tree: each span has a unique ``span_id`` and an optional
    ``parent_id`` linking to its parent span.

    Attributes:
        span_id: Unique identifier for this span.
        parent_id: The parent span's ID, or None for root spans.
        name: Human-readable operation name (e.g. "agent.execute").
        agent_id: The agent this span belongs to.
        attributes: Arbitrary key-value metadata.
        start_time: Monotonic start timestamp.
        end_time: Monotonic end timestamp (set by :meth:`end`).
        duration_ms: Duration in milliseconds (available after :meth:`end`).
    """

    def __init__(
        self,
        name: str,
        agent_id: str | None = None,
        parent_id: str | None = None,
    ) -> None:
        self.span_id: str = uuid.uuid4().hex[:16]
        self.parent_id: str | None = parent_id
        self.name: str = name
        self.agent_id: str | None = agent_id
        self.attributes: dict[str, Any] = {}
        self.start_time: float = time.monotonic()
        self.end_time: float | None = None
        self.status: str = "ok"
        self.error: str | None = None

    @property
    def duration_ms(self) -> int | None:
        if self.end_time is None:
            return None
        return int((self.end_time - self.start_time) * 1000)

    def set_attribute(self, key: str, value: Any) -> None:
        self.attributes[key] = value

    def set_error(self, error: str) -> None:
        self.status = "error"
        self.error = error

    def end(self) -> None:
        if self.end_time is None:
            self.end_time = time.monotonic()

    def to_dict(self) -> dict[str, Any]:
        return {
            "span_id": self.span_id,
            "parent_id": self.parent_id,
            "name": self.name,
            "agent_id": self.agent_id,
            "status": self.status,
            "error": self.error,
            "duration_ms": self.duration_ms,
            "attributes": dict(self.attributes),
        }
```

**Step 4: Implement Tracer + TracingCallbackHandler**

```python
# src/openclaw_sdk/tracing/tracer.py
"""Tracer — collect hierarchical execution traces."""
from __future__ import annotations

from typing import Any

from openclaw_sdk.callbacks.handler import CallbackHandler
from openclaw_sdk.core.types import ExecutionResult, StreamEvent, TokenUsage
from openclaw_sdk.tracing.span import Span


class Tracer:
    """Collects execution spans into hierarchical traces.

    Use with :class:`TracingCallbackHandler` for automatic tracing::

        tracer = Tracer()
        handler = TracingCallbackHandler(tracer)
        result = await agent.execute("hello", callbacks=[handler])
        print(tracer.export_json())
    """

    def __init__(self) -> None:
        self._spans: list[Span] = []

    def start_span(
        self,
        name: str,
        agent_id: str | None = None,
        parent: Span | None = None,
        **attributes: Any,
    ) -> Span:
        span = Span(
            name=name,
            agent_id=agent_id,
            parent_id=parent.span_id if parent else None,
        )
        for k, v in attributes.items():
            span.set_attribute(k, v)
        return span

    def end_span(self, span: Span) -> None:
        span.end()
        self._spans.append(span)

    def get_traces(self) -> list[Span]:
        return list(self._spans)

    def export_json(self) -> list[dict[str, Any]]:
        return [s.to_dict() for s in self._spans]

    def clear(self) -> None:
        self._spans.clear()


class TracingCallbackHandler(CallbackHandler):
    """Callback handler that automatically creates spans for execution traces.

    Each ``on_execution_start`` creates a root span; ``on_execution_end`` closes it.
    Tool calls and LLM calls create child spans.
    """

    def __init__(self, tracer: Tracer) -> None:
        self._tracer = tracer
        self._active_spans: dict[str, Span] = {}  # agent_id -> root span

    async def on_execution_start(self, agent_id: str, query: str) -> None:
        span = self._tracer.start_span(
            "agent.execute", agent_id=agent_id, query=query
        )
        self._active_spans[agent_id] = span

    async def on_tool_call(self, agent_id: str, tool_name: str, tool_input: str) -> None:
        parent = self._active_spans.get(agent_id)
        span = self._tracer.start_span(
            f"tool.{tool_name}", agent_id=agent_id, parent=parent, input=tool_input
        )
        # Store tool span for on_tool_result
        self._active_spans[f"{agent_id}:tool:{tool_name}"] = span

    async def on_tool_result(
        self, agent_id: str, tool_name: str, result: str, duration_ms: int
    ) -> None:
        key = f"{agent_id}:tool:{tool_name}"
        span = self._active_spans.pop(key, None)
        if span:
            span.set_attribute("duration_ms", duration_ms)
            self._tracer.end_span(span)

    async def on_execution_end(self, agent_id: str, result: ExecutionResult) -> None:
        span = self._active_spans.pop(agent_id, None)
        if span:
            span.set_attribute("latency_ms", result.latency_ms)
            span.set_attribute("success", result.success)
            span.set_attribute("tokens.input", result.token_usage.input)
            span.set_attribute("tokens.output", result.token_usage.output)
            self._tracer.end_span(span)

    async def on_error(self, agent_id: str, error: Exception) -> None:
        span = self._active_spans.pop(agent_id, None)
        if span:
            span.set_error(str(error))
            self._tracer.end_span(span)
```

**Step 5: Run tests, verify pass**

Run: `python -m pytest tests/unit/test_tracing.py -v`

**Step 6: Commit**

```bash
git add src/openclaw_sdk/tracing/ tests/unit/test_tracing.py
git commit -m "feat: add Tracer with hierarchical spans + TracingCallbackHandler"
```

---

### Task 6: Prompt Templates

**Priority:** HIGH — enables composable, reusable prompt construction.

**Files:**
- Create: `src/openclaw_sdk/prompts/` (new module)
- Create: `src/openclaw_sdk/prompts/__init__.py`
- Create: `src/openclaw_sdk/prompts/template.py`
- Test: `tests/unit/test_prompts.py`

**Step 1: Write the failing test**

```python
# tests/unit/test_prompts.py
import pytest
from openclaw_sdk.prompts.template import PromptTemplate


def test_basic_template():
    t = PromptTemplate("Hello, {name}!")
    assert t.render(name="World") == "Hello, World!"


def test_template_with_multiple_vars():
    t = PromptTemplate("Summarise {topic} in {style} style")
    result = t.render(topic="AI", style="academic")
    assert result == "Summarise AI in academic style"


def test_template_missing_variable_raises():
    t = PromptTemplate("Hello, {name}!")
    with pytest.raises(KeyError):
        t.render()


def test_template_variables_property():
    t = PromptTemplate("Research {topic} and write in {style}")
    assert t.variables == {"topic", "style"}


def test_template_compose():
    system = PromptTemplate("You are a {role}.")
    user = PromptTemplate("Analyse: {query}")
    combined = system + "\n\n" + user
    result = combined.render(role="analyst", query="market trends")
    assert result == "You are a analyst.\n\nAnalyse: market trends"


def test_template_partial():
    t = PromptTemplate("Hello {name}, welcome to {place}!")
    partial = t.partial(place="OpenClaw")
    assert partial.render(name="Alice") == "Hello Alice, welcome to OpenClaw!"
```

**Step 2: Run test to verify it fails**

**Step 3: Implement PromptTemplate**

```python
# src/openclaw_sdk/prompts/template.py
"""Composable prompt templates with variable substitution."""
from __future__ import annotations

import re
from typing import Any


class PromptTemplate:
    """A reusable prompt template with ``{variable}`` placeholders.

    Example::

        t = PromptTemplate("Research {topic} and write {style}")
        prompt = t.render(topic="AI safety", style="concisely")

    Templates can be composed with ``+``::

        system = PromptTemplate("You are a {role}.")
        user = PromptTemplate("Task: {query}")
        combined = system + "\\n\\n" + user
    """

    _VAR_PATTERN = re.compile(r"\{(\w+)\}")

    def __init__(self, template: str, **defaults: str) -> None:
        self._template = template
        self._defaults: dict[str, str] = dict(defaults)

    @property
    def template(self) -> str:
        return self._template

    @property
    def variables(self) -> set[str]:
        return set(self._VAR_PATTERN.findall(self._template))

    def render(self, **kwargs: str) -> str:
        merged = {**self._defaults, **kwargs}
        return self._template.format(**merged)

    def partial(self, **kwargs: str) -> PromptTemplate:
        new_defaults = {**self._defaults, **kwargs}
        return PromptTemplate(self._template, **new_defaults)

    def __add__(self, other: PromptTemplate | str) -> PromptTemplate:
        if isinstance(other, PromptTemplate):
            merged_defaults = {**self._defaults, **other._defaults}
            return PromptTemplate(
                self._template + other._template, **merged_defaults
            )
        return PromptTemplate(
            self._template + other, **self._defaults
        )

    def __radd__(self, other: str) -> PromptTemplate:
        return PromptTemplate(other + self._template, **self._defaults)

    def __repr__(self) -> str:
        return f"PromptTemplate({self._template!r})"
```

**Step 4: Run tests, verify pass**

**Step 5: Commit**

```bash
git add src/openclaw_sdk/prompts/ tests/unit/test_prompts.py
git commit -m "feat: add PromptTemplate with compose, partial, variables"
```

---

### Task 7: Batch Execution

**Priority:** HIGH — run multiple queries in parallel, critical for production workloads.

**Files:**
- Modify: `src/openclaw_sdk/core/agent.py` (add `batch` method)
- Test: `tests/unit/test_batch.py`

**Step 1: Write the failing test**

```python
# tests/unit/test_batch.py
import pytest
from openclaw_sdk import OpenClawClient, ClientConfig
from openclaw_sdk.core.constants import EventType
from openclaw_sdk.core.types import StreamEvent
from openclaw_sdk.gateway.mock import MockGateway


def _done(content: str) -> StreamEvent:
    return StreamEvent(
        event_type=EventType.DONE,
        data={"payload": {"runId": "run1", "content": content}},
    )


async def test_batch_returns_multiple_results():
    mock = MockGateway()
    await mock.connect()
    mock.register("chat.send", {"runId": "run1", "status": "started"})
    for i in range(3):
        mock.emit_event(_done(f"response {i}"))

    client = OpenClawClient(config=ClientConfig(), gateway=mock)
    agent = client.get_agent("bot")
    results = await agent.batch(["query 0", "query 1", "query 2"])
    assert len(results) == 3
    for r in results:
        assert r.success


async def test_batch_with_max_concurrency():
    mock = MockGateway()
    await mock.connect()
    mock.register("chat.send", {"runId": "run1", "status": "started"})
    for i in range(2):
        mock.emit_event(_done(f"response {i}"))

    client = OpenClawClient(config=ClientConfig(), gateway=mock)
    agent = client.get_agent("bot")
    results = await agent.batch(["q1", "q2"], max_concurrency=1)
    assert len(results) == 2
```

**Step 2: Run test to verify it fails**

**Step 3: Implement Agent.batch()**

Add to `core/agent.py`:

```python
async def batch(
    self,
    queries: list[str],
    options: ExecutionOptions | None = None,
    callbacks: list[CallbackHandler] | None = None,
    max_concurrency: int | None = None,
) -> list[ExecutionResult]:
    """Execute multiple queries in parallel.

    Args:
        queries: List of query strings to execute.
        options: Shared execution options for all queries.
        callbacks: Shared callbacks for all queries.
        max_concurrency: Max parallel executions (default: unlimited).

    Returns:
        List of :class:`ExecutionResult` in the same order as queries.
    """
    sem = asyncio.Semaphore(max_concurrency or len(queries))

    async def _run(query: str) -> ExecutionResult:
        async with sem:
            return await self.execute(query, options=options, callbacks=callbacks)

    return list(await asyncio.gather(*[_run(q) for q in queries]))
```

**Step 4: Run tests, verify pass**

**Step 5: Commit**

```bash
git add src/openclaw_sdk/core/agent.py tests/unit/test_batch.py
git commit -m "feat: add Agent.batch() for parallel query execution"
```

---

### Task 8: Evaluation Framework

**Priority:** HIGH — test agent quality systematically, catch regressions.

**Files:**
- Create: `src/openclaw_sdk/evaluation/` (new module)
- Create: `src/openclaw_sdk/evaluation/__init__.py`
- Create: `src/openclaw_sdk/evaluation/eval_suite.py`
- Create: `src/openclaw_sdk/evaluation/evaluators.py`
- Test: `tests/unit/test_evaluation.py`

**Step 1: Write the failing test**

```python
# tests/unit/test_evaluation.py
import pytest
from openclaw_sdk.evaluation.eval_suite import EvalCase, EvalResult, EvalSuite
from openclaw_sdk.evaluation.evaluators import ContainsEvaluator, ExactMatchEvaluator
from openclaw_sdk.core.types import ExecutionResult


async def test_contains_evaluator_pass():
    ev = ContainsEvaluator(expected="hello")
    result = ExecutionResult(success=True, content="hello world")
    assert ev.evaluate(result) is True


async def test_contains_evaluator_fail():
    ev = ContainsEvaluator(expected="goodbye")
    result = ExecutionResult(success=True, content="hello world")
    assert ev.evaluate(result) is False


async def test_exact_match_evaluator():
    ev = ExactMatchEvaluator(expected="42")
    result = ExecutionResult(success=True, content="42")
    assert ev.evaluate(result) is True


async def test_eval_suite_runs_cases():
    suite = EvalSuite(name="basic")
    suite.add_case(EvalCase(
        query="What is 2+2?",
        evaluator=ContainsEvaluator(expected="4"),
    ))
    suite.add_case(EvalCase(
        query="Say hello",
        evaluator=ContainsEvaluator(expected="hello"),
    ))

    # Mock agent execution
    results = [
        ExecutionResult(success=True, content="The answer is 4"),
        ExecutionResult(success=True, content="hello there!"),
    ]
    report = suite.evaluate(results)
    assert report.total == 2
    assert report.passed == 2
    assert report.pass_rate == 1.0
```

**Step 2: Run test to verify it fails**

**Step 3: Implement evaluators**

```python
# src/openclaw_sdk/evaluation/evaluators.py
"""Built-in evaluators for agent response quality."""
from __future__ import annotations

import re
from abc import ABC, abstractmethod

from openclaw_sdk.core.types import ExecutionResult


class Evaluator(ABC):
    """Base evaluator. Returns True if the result passes the check."""

    @abstractmethod
    def evaluate(self, result: ExecutionResult) -> bool: ...


class ContainsEvaluator(Evaluator):
    """Passes if the response contains the expected substring."""

    def __init__(self, expected: str, case_sensitive: bool = False) -> None:
        self._expected = expected
        self._case_sensitive = case_sensitive

    def evaluate(self, result: ExecutionResult) -> bool:
        content = result.content
        expected = self._expected
        if not self._case_sensitive:
            content = content.lower()
            expected = expected.lower()
        return expected in content


class ExactMatchEvaluator(Evaluator):
    """Passes if the response exactly matches the expected string."""

    def __init__(self, expected: str, strip: bool = True) -> None:
        self._expected = expected
        self._strip = strip

    def evaluate(self, result: ExecutionResult) -> bool:
        content = result.content.strip() if self._strip else result.content
        expected = self._expected.strip() if self._strip else self._expected
        return content == expected


class RegexEvaluator(Evaluator):
    """Passes if the response matches the regex pattern."""

    def __init__(self, pattern: str) -> None:
        self._pattern = re.compile(pattern)

    def evaluate(self, result: ExecutionResult) -> bool:
        return bool(self._pattern.search(result.content))


class LengthEvaluator(Evaluator):
    """Passes if the response length is within bounds."""

    def __init__(self, min_length: int = 0, max_length: int | None = None) -> None:
        self._min = min_length
        self._max = max_length

    def evaluate(self, result: ExecutionResult) -> bool:
        length = len(result.content)
        if length < self._min:
            return False
        if self._max is not None and length > self._max:
            return False
        return True
```

**Step 4: Implement EvalSuite**

```python
# src/openclaw_sdk/evaluation/eval_suite.py
"""Evaluation suite for systematic agent testing."""
from __future__ import annotations

from dataclasses import dataclass, field

from openclaw_sdk.core.types import ExecutionResult
from openclaw_sdk.evaluation.evaluators import Evaluator


@dataclass
class EvalCase:
    """A single evaluation test case."""
    query: str
    evaluator: Evaluator
    name: str | None = None
    tags: list[str] = field(default_factory=list)


@dataclass
class EvalCaseResult:
    """Result of running a single eval case."""
    case: EvalCase
    passed: bool
    result: ExecutionResult


@dataclass
class EvalReport:
    """Aggregated evaluation report."""
    name: str
    total: int
    passed: int
    failed: int
    case_results: list[EvalCaseResult]

    @property
    def pass_rate(self) -> float:
        return self.passed / self.total if self.total > 0 else 0.0


class EvalSuite:
    """Run a set of eval cases against agent responses.

    Example::

        suite = EvalSuite("quality-check")
        suite.add_case(EvalCase(
            query="What is 2+2?",
            evaluator=ContainsEvaluator(expected="4"),
        ))
        # Run agent on each case, collect results
        report = suite.evaluate(execution_results)
        print(f"Pass rate: {report.pass_rate:.0%}")
    """

    def __init__(self, name: str) -> None:
        self.name = name
        self._cases: list[EvalCase] = []

    def add_case(self, case: EvalCase) -> None:
        self._cases.append(case)

    def evaluate(self, results: list[ExecutionResult]) -> EvalReport:
        """Evaluate a list of results against the registered cases.

        Args:
            results: Execution results in the same order as cases.

        Returns:
            An :class:`EvalReport` with pass/fail counts and per-case details.
        """
        case_results: list[EvalCaseResult] = []
        for case, result in zip(self._cases, results):
            passed = case.evaluator.evaluate(result)
            case_results.append(EvalCaseResult(case=case, passed=passed, result=result))

        passed_count = sum(1 for cr in case_results if cr.passed)
        return EvalReport(
            name=self.name,
            total=len(case_results),
            passed=passed_count,
            failed=len(case_results) - passed_count,
            case_results=case_results,
        )

    async def run(
        self,
        agent: object,  # Agent-like with async execute(query) method
    ) -> EvalReport:
        """Run all cases against an agent and return the report.

        Args:
            agent: An object with an ``async execute(query: str) -> ExecutionResult`` method.
        """
        results: list[ExecutionResult] = []
        for case in self._cases:
            result = await agent.execute(case.query)  # type: ignore[attr-defined]
            results.append(result)
        return self.evaluate(results)
```

**Step 5: Run tests, verify pass**

Run: `python -m pytest tests/unit/test_evaluation.py -v`

**Step 6: Commit**

```bash
git add src/openclaw_sdk/evaluation/ tests/unit/test_evaluation.py
git commit -m "feat: add EvalSuite with Contains/ExactMatch/Regex/Length evaluators"
```

---

### Task 9: Update `__init__.py` exports + protocol docs + CHANGELOG

**Files:**
- Modify: `src/openclaw_sdk/__init__.py` (export new modules)
- Modify: `docs/protocol.md` (add exec.approval.resolve, agent.wait, device.*)
- Modify: `CHANGELOG.md` (v0.2.0 section)
- Modify: `STATUS.md` (update metrics)

**Step 1: Update exports**

Add to `__init__.py`:
- `from openclaw_sdk.cache.base import ResponseCache, InMemoryCache`
- `from openclaw_sdk.tracing.tracer import Tracer, TracingCallbackHandler`
- `from openclaw_sdk.tracing.span import Span`
- `from openclaw_sdk.prompts.template import PromptTemplate`
- `from openclaw_sdk.evaluation.eval_suite import EvalSuite, EvalCase, EvalReport`
- `from openclaw_sdk.evaluation.evaluators import ContainsEvaluator, ExactMatchEvaluator, RegexEvaluator, LengthEvaluator`
- `from openclaw_sdk.devices.manager import DeviceManager`

**Step 2: Update `__openclaw_compat__`**

```python
__openclaw_compat__ = {
    "min": "2026.2.0",
    "max_tested": "2026.2.19-2",  # latest as of 2026-02-21
}
```

**Step 3: Update docs/protocol.md**

Add newly verified methods:
- `exec.approval.resolve` — `{id, decision}` — approve/deny pending execution
- `agent.wait` — `{runId}` — wait for agent run completion
- `device.token.rotate` — `{deviceId, role}` — rotate device auth token
- `device.token.revoke` — `{deviceId, role}` — revoke device auth token
- `skills.bins` — `{}` — list skill binaries (requires elevated role)

**Step 4: Update CHANGELOG.md**

Add `[0.2.0]` section with all new features.

**Step 5: Run full quality gate**

```bash
python -m pytest tests/ -q
python -m mypy src/ --ignore-missing-imports
python -m ruff check src/ tests/
```

**Step 6: Commit**

```bash
git add src/openclaw_sdk/__init__.py docs/protocol.md CHANGELOG.md STATUS.md
git commit -m "docs: update exports, protocol docs, changelog for v0.2.0"
```

---

### Task 10: Integration test against live gateway

**Files:**
- Create: `tests/integration/test_live_v02.py`

Test the new verified methods against the live gateway:
- `exec.approval.resolve` (will fail with "approval not found" — that's fine, proves method exists)
- `agent.wait` (will fail with "run not found" — that's fine)
- `device.token.rotate` (needs valid device ID)
- `device.token.revoke` (needs valid device ID)

**Step 1: Write integration test**

```python
# tests/integration/test_live_v02.py
"""Integration tests for v0.2 gateway methods against live OpenClaw."""
import pytest
from openclaw_sdk.gateway.protocol import ProtocolGateway
from openclaw_sdk.core.exceptions import GatewayError

pytestmark = pytest.mark.integration


@pytest.fixture
async def gateway():
    gw = ProtocolGateway(ws_url="ws://127.0.0.1:18789/gateway")
    try:
        await gw.connect()
        health = await gw.health()
        if not health.healthy:
            pytest.skip("Gateway not healthy")
    except Exception:
        pytest.skip("Gateway not reachable")
    yield gw
    await gw.close()


async def test_agent_wait_method_exists(gateway):
    """agent.wait should exist — returns param validation error, not 'unknown method'."""
    with pytest.raises(GatewayError) as exc_info:
        await gateway.call("agent.wait", {})
    assert "unknown method" not in str(exc_info.value).lower()


async def test_exec_approval_resolve_method_exists(gateway):
    """exec.approval.resolve should exist — returns param error, not 'unknown method'."""
    with pytest.raises(GatewayError) as exc_info:
        await gateway.call("exec.approval.resolve", {})
    assert "unknown method" not in str(exc_info.value).lower()


async def test_device_token_rotate_method_exists(gateway):
    """device.token.rotate should exist."""
    with pytest.raises(GatewayError) as exc_info:
        await gateway.call("device.token.rotate", {})
    assert "unknown method" not in str(exc_info.value).lower()
```

**Step 2: Run integration tests**

```bash
python -m pytest tests/integration/test_live_v02.py -v -m integration
```

**Step 3: Commit**

```bash
git add tests/integration/test_live_v02.py
git commit -m "test: add v0.2 live gateway integration tests"
```

---

## Execution Order

| Order | Task | Depends On | Estimated Effort |
|-------|------|------------|-----------------|
| 1 | Fix ApprovalManager | None | Small |
| 2 | Add agent.wait | None | Small |
| 3 | Add DeviceManager | None | Small |
| 4 | Response Cache | None | Medium |
| 5 | Tracing / Observability | None | Medium |
| 6 | Prompt Templates | None | Small |
| 7 | Batch Execution | None | Small |
| 8 | Evaluation Framework | None | Medium |
| 9 | Exports + docs + changelog | Tasks 1-8 | Small |
| 10 | Integration tests | Tasks 1-3 | Small |

Tasks 1-8 are fully independent and can run in parallel.
Task 9 depends on all others (export + docs).
Task 10 depends on Tasks 1-3 (new gateway methods).

---

## Quality Gates (must pass before v0.2.0 tag)

- [ ] `python -m pytest tests/ -q` — all tests pass
- [ ] `python -m mypy src/ --ignore-missing-imports` — zero errors
- [ ] `python -m ruff check src/ tests/` — zero issues
- [ ] Coverage >= 95%
- [ ] All 5 new gateway methods verified against live OpenClaw
- [ ] All 10 original examples still run
- [ ] `exec.approval.resolve` works (not NotImplementedError)
- [ ] Cache hit/miss/TTL/eviction tested
- [ ] Tracer produces valid hierarchical span trees
- [ ] EvalSuite runs against MockGateway
- [ ] CHANGELOG.md updated with v0.2.0 section
- [ ] `__openclaw_compat__.max_tested` updated
